---
title: "C.P.E. Bach - Portfolio Computational Musicology"
author: "Herma Ardesch"
date: "27-3-2021"
output: 
  flexdashboard::flex_dashboard:
   storyboard: true
   theme: yeti
   vertical layout: fill
---

## Authentic vs. Modern Performances of C.P.E. Bach's music

```{r}

library(tidyverse)
library(spotifyr)
library(compmus)
get_conf_mat <- function(fit) {
  outcome <- .get_tune_outcome_names(fit)
  fit %>% 
    collect_predictions() %>% 
    conf_mat(truth = outcome, estimate = .pred_class)
}  

get_pr <- function(fit) {
  fit %>% 
    conf_mat_resampled() %>% 
    group_by(Prediction) %>% mutate(precision = Freq / sum(Freq)) %>% 
    group_by(Truth) %>% mutate(recall = Freq / sum(Freq)) %>% 
    ungroup() %>% filter(Prediction == Truth) %>% 
    select(class = Prediction, precision, recall)
}  
library(plotly)
library(tidymodels)
library(kknn)
library(C50)
library(randomForest)
library(ggdendro)

```




### **Why C.P.E. Bach?** Discover more about the background of this portfolio. 

#### **The music of Carl Philipp Emanuel Bach** ####

Raised in an environment in which Bach's music was predominant, I learned to play the flute and developed a preference for the music of Carl Philipp Emanuel Bach (1714-1788). His 'Empfindsame Stil' with many unexpected melodic, harmonic and rhythmic turns to symbolize mood swings was (and still is) very appealing to me. This corpus consists of his flute concertos, cello concertos, some of his harpsichord sonatas and his four symphonies.

I would like to answer the question **Is it possible to detect differences between authentic and modern performances in the music of C.P.E. Bach, using the Spotify API?** I expect differences in tempo, interpretation, instrumental sound, pitch, valence and energy between authentic and modern performances. Of course, I need to give extra attention to the pitch of both groups, since authentic instruments usually play in a different tuning, resulting in a lower pitch. My objective is to make these differences **visible in graphs.**

Listening to his Flute Concerto in A minor, Wq 166 in the authentic performance of Konrad Hünteler with the Amsterdam Baroque Orchestra (Ton Koopman) and the modern performance of Emmanuel Pahud with the Kammerakademie Potsdam (Trevor Pinnock), you can hear the differences. Both performances take a different tempo, energy level and pitch and the wooden baroque traverso has quite a different sound character, compared to the modern silver or golden Böhm-flute. However, I expect that since the inception of the authentic performance practice over half a century ago, both authentic and modern performances have grown closer to one another, because of improved replicas of authentic instruments on the one hand and a greater versatility of instrumentalists playing both authentic and modern versions of their instrument. 

Links to the <a href="https://open.spotify.com/playlist/2PmIxISPPF4ymvoa3bvCrh?si=UQbGeBYpRmSKUpqAj1W_ow" target="_blank"> playlist used for authentic performances</a> and the <a href="https://open.spotify.com/playlist/0F83MSl8bV2AxL2JNDTGew?si=n_uQMkPtSTG9tQ3JuYgJ0w" target="_blank"> playlist used for modern performances.</a>

![](/2021 lopende zaken/studie/data/portfolio/pics CPE.jpg) 




### Authentic performances show a greater **diversity in tempo** than modern performances. 

```{r}

CPE_aut <- get_playlist_audio_features("", "2PmIxISPPF4ymvoa3bvCrh")
CPE_mod <- get_playlist_audio_features("", "0F83MSl8bV2AxL2JNDTGew")
CPE <-
  bind_rows(
    CPE_aut %>% mutate(category = "Authentic"),
    CPE_mod %>% mutate(category = "Modern"),
  )

tempo_gg <- CPE %>% ggplot(aes(x = tempo)) + 
 geom_histogram(binwidth = 8, color = "white", fill = "deepskyblue4") +
  facet_wrap(~category) +
  labs(title = "Differences in Tempo of Authentic vs. Modern Performances", size = 16
  ) +
  theme_minimal() +
  theme(rect = element_blank())

ggplotly(tempo_gg)

```

> Source: Spotify API

*** 

*Moving your cursor over the top right side of the histograms will show you an interactive menu. You can zoom in on the bins to see how often each tempo is used.*

The overall tempo of authentic performances in this corpus ranges from 62 to 176 BPM, the tempo of modern performances had a slightly lower range: from 62 to 168 BPM. However, as you see on the histogram, the tempo of the authentic performances has a more even, normal distribution, whereas that of the modern performances shows a high peak around 80 BPM. 

As much as 11 of the total of 45 movements are played in that same tempo of around 80 BPM in the modern performances, against 4 in the authentic performances. Based on the comparison of these **45 tracks,** you could say that authentic performances show a **greater variety in tempo** than modern performances.

### **Tempograms** of a Presto show **clear overall tempi** with **regular vague areas** in authentic and modern performances.

![comparing tempograms](/2021 lopende zaken/studie/data/portfolio/tempograms larger.jpg)


***

The cyclic tempogram of the authentic performance of a Presto from the Sinfonia in D-major played by The English concert is compared to that of the modern performance by Musikcollegium Winterthur. Overall, the authentic performance takes a slightly slower tempo of 120 BPM, whereas the modern performance is played at 126 BPM, as shown by the yellow lines on the tempograms.

The tempograms also show the **structure of the movement.** Interesting to see is that both tempograms have a vague, vertical column in the A, A1, B and B1 sections, in which it is difficult to estimate the tempo: The first one is marked as 'strings'. In the tempo curve you see that note onsets are hard to detect here, because only strings (without a high amplitude attack phase) play in a lower tempo and lower dynamics. Each of these columns has a long rest before, after and in the middle of it. The rests last 3-5 beats (in the 3/8 meter). They seem to cause spurious peaks on the tempo curve, similar to the high peaks at the beginning of each performance.

Apart from the slower, less loud and more reflective parts in this fast movement, Spotify does a good job in estimating the tempo ánd showing the structure.


### **Spotify mood features** show that authentic performances can sound relatively  **'fast, loud and noisy!'**. 

```{r}

mood_gg <- CPE %>% 
  ggplot(aes(x = valence, y = energy, color = category)) +
  geom_jitter() + geom_smooth() +
  labs(title = "Different Moods in Authentic vs. Modern Performances", colour = "Category") +
  theme_minimal() +
  theme(rect = element_blank())

ggplotly(mood_gg)

```
> Source: Spotify API

***

Here we see valence and energy values, resulting in moods per track. Both authentic and modern performances range in mood from the lower left 'sad' quadrant to the middle of the lower right 'calm' quadrant. This genre will never reach the upper right 'happy' quadrant (energy level 0.5 - 1.0), although the valence  (negative - positive) of authentic performances uses the full range.

The authentic performances show a slightly higher energy level, except for the beginning and the middle part. Spotify calculates the energy level using a mix of features and mentions that *'energetic tracks feel fast, loud and noisy'*. The lines crossing in the middle may be caused by the strong preference of tempi around 80 BPM in the modern performances (see: diversity in tempo).

**Two outliers** in this plot show a high valence and energy level in the two **fast movements of the Keyboard Sonata in A-Minor, Wq 57/II,** Allegro and Allegro di Molto, played on harpsichord by Gabor Antalffi. Listening to it, he plays them vigorously. But compared to the modern version of these movements played by Ana-Marija Markovina on the piano, the sound of harpsichord vs. piano certainly also plays a role in the Spotify high scores. The harpsichord certainly feels relatively faster, louder and noisier!

### **Comparing timbres** of an Allegro di Molto of a Keyboard Sonata played on **harpsichord vs. piano.**


![comparing timbres](/2021 lopende zaken/studie/data/portfolio/SSMs.jpg)

***

The harpsichord outliers in the previous plot (mood features) deserve extra attention. Let's compare the timbre of the harpsichord to that of the piano in two Timbre Self-Similarity Matrices (SSMs) of the Allegro di Molto. An SSM compares a track to itself, resulting in a visual representation mirrored on the diagonal.

Both SSMs show a **similar structure**: The diagonal lines parallel to the central diagonal represent comparable repetitions, the yellow lines show novelties in structure. F.i. at 100 (left) and 160 seconds (right) the first section closes with a fermata and a caesura after which the last section starts. The checkerboard pattern of the piano version is clearer and more homogenous. This may be caused by Markinova playing it in a steady, almost mechanical tempo, accented primarily by subtle dynamics. The mechanism of the harpsichord doesn't allow for dynamics, so Antalffi uses strong agogic accentuation and longer caesuras than Markovina's: Antalffi's **high expressivity** and his faster performance might account for the blurrier lines of the harpsichord SSM.

Although it is difficult to analyze the timbre based on these two SSMs, they look different. The penetrating sound of the harpsichord with its plucked strings versus the rounder, warmer sound of the piano also result in visual timbre differences.

### **Different pitches** of authentic vs. modern performances clearly visible in **chromagrams** from Trio Sonata.

![comparing pitches](/2021 lopende zaken/studie/data/portfolio/chromagram pitches.jpg)

***

These two chromagrams represent two performancese of the Adagio from the Trio Sonata for flute, violin and basso continuo, Wq 143, on the left performed on authentic instruments by Le Nouveau Quatuor vs. a modern performance on the right, played by Batiashvili, Pahud, Klinger and Koffer. 

A chromagram shows the pitches of the track as you can see on the y-axis. This Trio Sonata is written in B-Minor, consisting of the notes b, c-sharp, d, e, f-sharp, g, a. The triad b - d - f-sharp defines the basic identity of this key. In a chromagram you can usually identify the basic key identity by the lines that contain more yellow, although modulations of course accentuate other notes and C.P.E. Bach is well-known for his sudden key-changes.

The right chromagram shows the B-Minor identity clearly, although the e-line (the subdominant) also stands out. More important is that you see on the left chromagram that the lines **shift a semitone downwards.** This shows the lower pitch in which authentic instruments play: more or less a semitone lower than the modern concert pitch. So the difference in pitch is nicely identified by Spotify!

### Let's put **Spotify's chord and key analysis** to the test with the original Krumhansl-Schmuckler templates!

![chord and key analysis](/2021 lopende zaken/studie/data/portfolio/chordograms hr.jpg)


***

The Adagio from the Trio Sonata Wq 143 from the previous tab (different pitches) is used here in its modern performance. I chose a a relatively short, slow piece, because C.P.E. Bach is well-known for his capricious harmonic changes and this might be hard enough!

The upper left chordogram is the most detailed one here: chords are taken on measure level to show their frequent changes - although a beat level would definitely show even more! The first measure **in the score** starts in Bm, goes to C#m with an added #6, to F#m and back to Bm on the first beat of the second measure. It looks like **Spotify** averages the first measure by analyzing it as an A7 chord, which shares c# - e - g with the C#m chord and a - c# with the F#m. Also on  section level the Spotify analysis remains fuzzy and indetermined.

The keygrams are also unclear or incorrect, except for one clear F#m key in the second section. This is the part where the violin enters and takes over the theme of the flute, now in the dominant. Here Spotify takes the complete section as the F#m key, the dominant of the original Bm key, so this is correct. But all the accidentals, chromatic runs and key-changes obviously are too much for Spotify. 


### **Dynamic Time-Warping** to align a Concerto in the version for Flute to the version for Cello, played in a **much slower tempo.** 

![dynamic time warping](/2021 lopende zaken/studie/data/portfolio/DTW.jpg)


***

C.P.E. Bach's **Flute Concerto in A, Wq 168,** is also available as a **Cello Concerto, Wq 172.** This chromagram shows the second movement, a Largo con Sordini, Mesto, so a slow piece. They are performed by Amsterdam Baroque with Konrad Hünteler on flute and by the Bach Collegium Japan with Hidemi Suzuki on cello. Amsterdam Baroque plays this movement in 6:43 minutes - the Bach Collegium Japan takes a much slower tempo: 8:33 minutes. 

This chromagram is made using the Dynamic Time-Warping technique (DTW), using Manhattan normalization and Aitchison distance to align both versions. DTW is a great help if you want to compare various performances. You can see by the longer x-axis that the Bach Collegium Japan plays much slower, resulting in a rectangular chromagram.

From the bottom left corner to the upper right corner you see a line, showing the warping path. A straight line shows that both movements are well aligned. When you look close, you see a small discontinuation of the line towards the end of the movement. This is caused by a cadenza: the flute plays a short cadenza of 16 seconds and the cello plays a more extensive cadenza of 32 seconds - there is no alignment possible here. 


### **Classification** of the corpus by Spotify using the **RandomForest Model** shows promising precision!

![Classification and Prediction](/2021 lopende zaken/studie/data/portfolio/classification and prediction.jpg)

***

The previous tabs showed visualisations, based on the Spotify API. Now let's find out if Spotify can classify the works of this corpus in the right category of authentic or modern performances. Not an easy task, considering the variety of works!

First we looked for the most important features Spotify uses for this classification task and found that all twelve timbre features were on top of the list. Using these twelve timbre features, a prediction was calculated - you find it in the confusion matrix. Then the complete corpus was divided into five parts, using the RandomForest Model for cross-validation and to train Spotify to perform better in each run. The calculated **precision** was:

| 
| ------------- | ------------- |
| **Authentic vs.**    | **Modern**        |
| 86%           | 85%           |
| 86%           | 86%           |
| 88%           | 83% (!)       |
| 89%           | 87%           |
| 


Although the number of cross-validation runs is limited, the precision is promising and it looks like Spotify can learn to classify performances with more precision! 

The **scatterplot shows two timbre components** that played an important role in this task.


### **Overview and Conclusion** - Spotify API audio retrieval also useful for **classical music!**

>

#### **Overview of Findings, External Validity and Conclusion** ####

In this portfolio two playlists of each 45 performances of C.P.E. Bach's works were compared: one playlist with performances on authentic instruments and the second playlist consisting of the same pieces performed on modern instruments. Comparisons were made using the Spotify API. I expected to find differences in pitch, tempo, timbre, interpretation and mood (valence - energy).

In the visualisations on **playlist level** you can see that tempi in both groups were different indeed, the most conspicuous difference being a greater variety of tempo in the authentic performances. I also found a slightly greater variety (mainly because of two outliers) in comparing mood (valence - energy) in the authentic performances. This is a very important aspect, because C.P.E. Bach instructed performers of his music *'to be moved themselves, in order to transfer the emotions expressed in the music to the audience'* (quote from *'Versuch über die Wahre Art das Clavier zu spielen'*, C.P.E. Bach). 

On **track level** two tempograms and short tempo curves of two individual tracks were compared. They showed not only the tempi, but also the structures of the compared works. In two chromagrams Spotify nicely identified the lower pitch authentic instruments use, compared to modern instruments. Two outliers in the mood plot were further analyzed in two timbre Self Similarity Matrices. They not only showed that the harpsichord's relative loudness results in quite a different timbre than that of the piano, but also that the different mechanics require a different interpretation technique. 

The only analyses that Spotify did not perform correctly were key and chord identification. However, for this purpose I only used the minor, major and dominant seventh chord and key templates to match the audio. Since C.P.E. Bach uses many, often unexpected harmonic turns that are more complicated than the templates used, this is not surprising. Key and chord analysis of this specific style might however be a research topic to explore further, for instance in a machine-learning study to familiarize the computer with the capricious harmonic idiom of C.P.E. Bach.

A bit beside the research question, but a basic technique used in apps that allow for switching between different performances and the score, I added a Dynamic Time Warping plot as a bonus. For this purpose I used two versions of the same piece with a quite different duration from the authentic playlist and showed how DTW can align them. Finally, I was nicely surprised by the promising precision of the Spotify API in the classification of performances and the possibility to improve this even further by training! 

The **external validity** of this research has two aspects.

1. C.P.E. Bach's music represents the **short transition of two style periods: from baroque to classicism.** This makes it a very specific style and limits the music to a relatively short period in time. This specificity has a direct influence on the external validity of the findings of the musical style in this portfolio: It is not really possible to generalize them, simply because C.P.E. Bach is the most important composer of the 'Empfindsame Stil'. 

2. The portfolio focused not only on this style, but also on **comparing authentic performances to modern performances**. This comparison has a much broader scope than C.P.E. Bach's music. Especially because the Spotify API does a relatively good job to classify the two performance styles, the external validity of this research aspect is of greater importance and so this study ánd the Spotify API might be useful for anyone comparing authentic to modern performances.

**Concluding**, I was able to research *more* than the differences I expected to find using the Spotify API. The graphs in this portfolio give an immediate insight in all the results. Working on this portfolio made me realize that the Spotify API can be a great help to retrieve audio information on several levels, also for classical music! Visualizing these data in R can be a great help to easily understand what is happening in a musical performance.
