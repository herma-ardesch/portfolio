---
title: "C.P.E. Bach"
author: "Herma Ardesch"
date: "6-3-2021"
output: 
  flexdashboard::flex_dashboard:
   storyboard: true
   theme: yeti
   vertical layout: fill
---

## Authentic vs. Modern Performances of C.P.E. Bach's music

```{r}

library(tidyverse)
library(spotifyr)
library(compmus)
library(plotly)

```


### Let's put **Spotify's chord and key analysis** to the test!

![chord and key analysis](/2021 lopende zaken/studie/data/portfolio/chordograms.jpg)


***

The Adagio from the Trio Sonata Wq 143 from the previous tab (different pitches) is used here in the modern performance. I chose a a relatively short, slow piece, because C.P.E. Bach is well-known for his capricious harmonic changes and this might be hard enough!

The upper left chordogram is the most detailed one here: chords are taken on measure level to show their frequent changes - although a beat level would definitely show even more! The first measure **in the score** starts in Bm, goes to C#m with an added #6, to F#m and back to Bm on the first beat of the second measure. It looks like **Spotify** averages the first measure by analyzing it as an A7 chord, which shares c# - e - g with the C#m chord and a - c# with the F#m. Also on  section level the Spotify analysis remains fuzzy and indetermined.

The keygrams are also unclear except for one clear F#m key in the second section. This is the part where the violin enters and takes over the theme of the flute, now in the dominant. Here Spotify takes the complete section as the F#m key, the dominant of the original Bm key, so this is correct. But all the accidentals, chromatic runs and key-changes obviously are too much for Spotify. 


### **Why C.P.E. Bach?** Discover more about the background of this portfolio. 

#### **The music of Carl Philipp Emanuel Bach** ####

Raised in an environment in which Bach's music was predominant, I learned to play the flute and developed a preference for the music of Carl Philipp Emanuel Bach (1714-1788). His 'Empfindsame Stil' with many unexpected melodic, harmonic and rhythmic turns to symbolize mood swings was very appealing to me. This corpus consists of his flute concertos, cello concertos, some of his harpsichord sonatas and his four symphonies.

I would like to answer the question **Is it possible to detect differences between authentic and modern performances in the music of C.Ph.E. Bach, using the Spotify API?** I expect differences in tempo, interpretation, instrumental sound, pitch, valence and energy between authentic and modern performance practices. Of course I need to give extra attention to the pitch of both groups, since authentic instruments usually play in a different tuning, resulting in a lower pitch. My objective is to make these differences visible.

Listening to his Flute Concerto in A minor, Wq 166 in the authentic performance of Konrad Hünteler with the Amsterdam Baroque Orchestra (Ton Koopman) and the modern performance of Emmanuel Pahud with the Kammerakademie Potsdam (Trevor Pinnock), you can hear the differences. Both performances take a different tempo, energy level and pitch and the wooden baroque traverso has quite a different sound character, compared to the modern silver or golden Böhm-flute. 

However, I expect that since the inception of the authentic performance practice over half a century ago, both authentic and modern performances have grown closer to each other, because of improved replicas of authentic instruments on the one hand and a greater versatility of instrumentalists playing both authentic and modern versions of their instrument.
 

### Authentic performances show a greater **diversity in tempo** than modern performances. 

```{r}

CPE_aut <- get_playlist_audio_features("", "2PmIxISPPF4ymvoa3bvCrh")
CPE_mod <- get_playlist_audio_features("", "0F83MSl8bV2AxL2JNDTGew")
CPE <-
  bind_rows(
    CPE_aut %>% mutate(category = "Authentic"),
    CPE_mod %>% mutate(category = "Modern"),
  )

tempo_gg <- CPE %>% ggplot(aes(x = tempo)) + 
 geom_histogram(binwidth = 8, color = "white", fill = "deepskyblue4") +
  facet_wrap(~category) +
  labs(title = "Differences in tempo of authentic vs. modern performances", size = 16
  ) +
  theme(rect = element_blank())

ggplotly(tempo_gg)

```

> Source: Spotify API

*** 

*Moving your cursor over the top right side of the histograms will show you an interactive menu. You can zoom in on the bins to see how often each tempo is used.*

The overall tempo of authentic performances in this corpus ranges from 62 to 176 BPM, the tempo of modern performances had a slightly lower range: from 62 to 168 BPM. However, as you see on the histogram, the tempo of the authentic performances has a more even, normal distribution, whereas that of the modern performances shows a high peak around 80 BPM. 

As much as 11 of the total of 45 movements are played in that same tempo of around 80 BPM in the modern performances, against 4 in the authentic performances. Based on the comparison of these 45 tracks, you could say that authentic performances show a greater variety in tempo than modern performances.


### **Spotify mood features** show that authentic performances can sound relatively  **'fast, loud and noisy!'**. 

```{r}

mood_gg <- CPE %>% 
  ggplot(aes(x = valence, y = energy, color = category)) +
  geom_jitter() + geom_smooth() +
  labs(title = "Different moods in authentic vs. modern performances") +
  theme(rect = element_blank())

ggplotly(mood_gg)

```
> Source: Spotify API

***

Here we see the valence vs. energy values, resulting in moods per track. Both authentic and modern performances range in mood from the lower left 'sad' quadrant to the middle of the lower right 'calm' quadrant. This genre will never reach the upper right 'happy' quadrant (energy level 0.5 - 1.0), although its valence (negative - positive) uses the full range.

The authentic performances show a slightly higher energy level, except for the beginning and the middle part. Spotify calculates the energy level using a mix of features and mentions that *'energetic tracks feel fast, loud and noisy'*. The lines crossing in the middle may be caused by the strong preference of tempos around 80 BPM in the modern performances (see: diversity in tempo).

Two outliers in this plot show a high valence and energy level in the two fast movements of the Keyboard Sonata in A-Minor, I Allegro and III Allegro di Molto, played on harpsichord by Gabor Antalffi. Listening to it, he plays them vigorously. But compared to the modern version of these movements played by Ana-Marija Markovina on the piano, the sound of harpsichord vs. piano certainly also plays a role in the Spotify high scores. The harpsichord certainly feels relatively faster, louder and noisier!

### **Comparing timbres** of an Allegro di Molto of a Keyboard Sonata played on harpsichord vs. the same movement played on piano.


![comparing timbres](/2021 lopende zaken/studie/data/portfolio/SSMs.jpg)

***

The harpsichord outliers in the previous plot (mood features) deserve extra attention. Let's compare the timbre of the harpsichord to that of the piano in two Timbre Self-Similarity Matrices (SSMs) of the Allegro di Molto. An SSM compares a track to itself, resulting in a visual representation mirrored on the diagonal.

Both SSMs show a similar structure: the diagonal lines parallel to the central diagonal represent comparable repetitions, the yellow lines show novelties in structure. F.i. at 100 (left) and 160 seconds (right) the first section closes with a fermata and a ceasura after which the last section starts. The checkerboard pattern is clearer and more homogenous in the piano version. This may be caused by Markinova playing it in a steady, almost mechanical tempo, accented primarily by subtle dynamics. The mechanism of the harpsichord doesn't allow for dynamics, so Antalffi uses strong agogic accentuation and longer caesuras than Markinova's: Antalffi's high expressivity and his faster performance might account for the blurrier lines of the harpsichord SSM.

Although it is difficult to analyze the timbre based on these two SSM's, they look different. The penetrating sound of the harpsichord with its plucked strings versus the rounder, warmer sound of the piano also result in visual timbre differences.

### Chromagrams clearly show **different pitches** in Adagio from a Trio Sonata performed on authentic vs. modern instruments.

![comparing pitches](/2021 lopende zaken/studie/data/portfolio/chromagram pitches.jpg)

***

These two chromagrams represent two performancese of the Adagio from the Trio Sonata for flute, violin and basso continuo, Wq 143, on the left performed on authentic instruments by Le Nouveau Quatuor vs. a modern performance on the right, played by Batiashvili, Pahud, Klinger and Koffer. 

A chromagram shows the pitches of the track as you can see on the y-axis. This Trio Sonata is written in B-Minor, consisting of the notes b, c-sharp, d, e, f-sharp, g, a. The triad b - d - f-sharp defines the basic identity of this key. In a chromagram you can usually identify the basic key identity by the lines that contain more yellow, although modulations of course accentuate other notes and C.P.E. Bach is well-known for his sudden key-changes.

The right chromagram shows the B-Minor identity clearly, although the e-line (the subdominant) also stands out. More important is that you see on the left chromagram that the lines shift a semitone downwards. This shows the lower pitch in which authentic instruments play: more or less a semitone lower than the modern concert pitch. So the difference in pitch is nicely identified by Spotify!


### **Dynamic Time-Warping** to easily compare a Concerto in the version for Flute to the same Concerto played on Cello in a much slower tempo. 

```{r}

hunteler_Wq168II <-
  get_tidy_audio_analysis("05swCwLFtyRnOZdR25nP0d") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)
  
suzuki_wq172II <- 
  get_tidy_audio_analysis("6fdcIHXZb7WoUKjlsREe36") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)
```


```{r}
compmus_long_distance(
  suzuki_wq172II %>% mutate(pitches = map(pitches, compmus_normalise, "manhattan")),
  hunteler_Wq168II %>% mutate(pitches = map(pitches, compmus_normalise, "manhattan")),
  feature = pitches,
  method = "aitchison"
) %>%
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_equal() +
  labs(x = "Hidemi Suzuki and Bach Collegium Japan", y = "Konrad Hünteler and Amsterdam Baroque") +
  theme_minimal() +
  scale_fill_viridis_c(guide = NULL) +
  labs(title = "Aligning two performances played in a different tempo")
```


***

C.P.E. Bach's Flute Concerto in A, Wq 168, is also available as a Cello Concerto, Wq 172. This chromagram shows the second movement, a Largo con Sordini, Mesto, so a slow piece. They are performed by Amsterdam Baroque with Konrad Hünteler on flute and by the Bach Collegium Japan with Hidemi Suzuki on cello. Amsterdam Baroque plays this movement in 6:43 minutes - the Bach Collegium Japan takes a much slower tempo: 8:33 minutes. 

This chromagram is made using the Dynamic Time-Warping technique (DTW), using Manhattan normalization and Aitchison distance to align both versions. DTW is a great help if you want to compare various performances. You can see by the longer x-axis that the Bach Collegium Japan plays much slower, resulting in a rectangular chromagram.

From the bottom left corner to the upper right corner you see a line, showing the warping path. A straight line shows that both movements are well aligned. When you look close, you see a small discontinuation of the line towards the end of the movement. This is caused by a cadenza: the flute plays a short cadenza of 16 seconds and the cello plays a more extensive cadenza of 32 seconds - there is no alignment possible here. 

### **Conclusions - TO DO: adjust title depending on other visualisations**.

>

#### **Conclusions etc.** ####

Enter bodytext
