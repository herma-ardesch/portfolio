---
title: "C.P.E. Bach"
author: "Herma Ardesch"
date: "19-2-2021"
output: 
  flexdashboard::flex_dashboard:
   storyboard: true
   theme: yeti
   vertical layout: fill
---

## Authentic vs. Modern Performances of C.P.E. Bach's music

### **Why C.P.E. Bach?** Discover more about the background of this portfolio.

```{r}

```

> 

#### **The music of Carl Philipp Emanuel Bach** ####

Raised in an environment in which Bach's music was predominant, I learned to play the flute and developed a preference for the music of Carl Philipp Emanuel Bach (1714-1788). His 'Empfindsame Stil' with many unexpected melodic, harmonic and rhythmic turns to symbolize mood swings was very appealing to me. This corpus consists of his flute concertos, cello concertos, some of his harpsichord sonatas and his four symphonies.

I would like to answer the question **Is it possible to detect differences between authentic and modern performance in the music of C.Ph.E. Bach, using the Spotify API?** I expect differences in tempo, interpretation, instrumental sound, pitch, valence and energy between authentic and modern performance practices. Of course I need to give extra attention to the pitch of both groups, since authentic instruments usually play in a different tuning, resulting in a lower pitch. My objective is to make these differences visible.

Listening to his Flute Concerto in A minor, Wq 166 in the authentic performance of Konrad Hünteler with the Amsterdam Baroque Orchestra (Ton Koopman) and the modern performance of Emmanuel Pahud with the Kammerakademie Potsdam (Trevor Pinnock), you can hear the diffenrences. Both performances take a different tempo, energy level and pitch and the wooden baroque traverso has quite a different sound character, compared to the modern silver or golden Böhm-flute. 

However, I expect that since the inception of the authentic performance practice over half a century ago, both authentic and modern performances have grown closer to each other, because of improved replicas of authentic instruments on the one hand and a greater versatility of instrumentalists playing both authentic and modern versions of their instrument.
 

### Authentic performances show a greater **diversity in tempo** than modern performances. 

```{r}

library(tidyverse)
library(spotifyr)
library(compmus)
library(plotly)

CPE_aut <- get_playlist_audio_features("", "2PmIxISPPF4ymvoa3bvCrh")
CPE_mod <- get_playlist_audio_features("", "0F83MSl8bV2AxL2JNDTGew")
CPE <-
  bind_rows(
    CPE_aut %>% mutate(category = "Authentic"),
    CPE_mod %>% mutate(category = "Modern")
  )
tempo_gg <- CPE %>% ggplot(aes(x = tempo)) + 
 geom_histogram(binwidth = 8, color = "grey80", fill = "deepskyblue4") +
  facet_wrap(~category) +
  
labs(title = "Differences in tempo of authentic vs modern performances", size = 16
)


ggplotly(tempo_gg)

```

> Source: Spotify API

*** 

*Moving your cursor over the top right side of the histograms will show you an interactive menu. You can zoom in on the bins to see how often each tempo is used.*

The overall tempo of authentic performances in this corpus ranges from 62 to 176 BPM, the tempo of modern performances had a slightly lower range: from 62 to 168 BPM. However, as you see on the histogram, the tempo of the authentic performances has a more even, normal distribution, whereas that of the modern performances shows a high peak around 80 BPM. 

As much as 11 of the total of 45 movements are played in that same tempo of around 80 BPM in the modern performances, against 4 in the authentic performances. Based on the comparison of these 45 tracks, you could say that authentic performances show a greater variety in tempo than modern performances.


### **Spotify mood features** show that authentic performances can sound relatively  **'fast, loud and noisy!'**. 

```{r}

mood_gg <- CPE %>% ggplot(aes(x = valence, y = energy, color = category)) +
  geom_jitter() + geom_smooth() +
labs(title = "Differences in loudness of authentic vs modern performances")

ggplotly(mood_gg)

```

> Source: Spotify API

***

Here we see the valence vs. energy values, resulting in moods per track. Both authentic and modern performances range in mood from the lower left 'sad' quadrant to the lower part of the 'calm' quadrant. This genre will never reach the upper right 'happy' quadrant (energy level 0.5 - 1.0), although its valence (negative - positive) uses the full range.

The authentic performances show a slightly higher energy level, except for the beginning and the middle part. Spotify calculates the energy level using a mix of features and mentions that *'energetic tracks feel fast, loud and noisy'*. The lines crossing in the middle may be caused by the strong preference of tempos around 80 BPM in the modern performances (TO DO: add link to tempo).

Two outliers in this plot show a high valence and energy level in the two fast movements of the Keyboard Sonata in A-Minor, I Allegro and III Allegro di Molto, played on harpsichord by Gabor Antalffi. Listening to it, he plays them vigorously. But compared to the modern version of these movements played by Ana-Marija Markovina on the piano, the sound of harpsichord vs. piano certainly also plays a role in the Spotify high scores. The harpsichord certainly feels relatively faster, louder and noisier!


### **Dynamic Time-Warping technique** to easily compare a Concerto in the version for Flute to the same Concerto played on Cello in a much slower tempo. 

```{r}

hunteler_Wq168II <-
  get_tidy_audio_analysis("05swCwLFtyRnOZdR25nP0d") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)
  
suzuki_wq172II <- 
  get_tidy_audio_analysis("6fdcIHXZb7WoUKjlsREe36") %>%
  select(segments) %>%
  unnest(segments) %>%
  select(start, duration, pitches)
```


```{r}
compmus_long_distance(
  hunteler_Wq168II %>% mutate(pitches = map(pitches, compmus_normalise, "manhattan")),
  suzuki_wq172II %>% mutate(pitches = map(pitches, compmus_normalise, "manhattan")),
  feature = pitches,
  method = "aitchison"
) %>%
  ggplot(
    aes(
      x = xstart + xduration / 2,
      width = xduration,
      y = ystart + yduration / 2,
      height = yduration,
      fill = d
    )
  ) +
  geom_tile() +
  coord_equal() +
  labs(x = "Konrad Hünteler and Amsterdam Baroque", y = "Hidemi Suzuki and Bach Collegium Japan") +
  theme_minimal() +
  scale_fill_viridis_c(guide = NULL) +
  labs(title = "Aligning two performances played in a different tempo")
```


***

C.P.E. Bach's Flute Concerto in A, Wq 168, is also available as a Cello Concerto, Wq 172. This chromagram shows the second movement, a Largo con Sordini, Mesto, so a slow piece. They are performed by Amsterdam Baroque with Konrad Hünteler on flute and by the Bach Collegium Japan with Hidemi Suzuki on cello. Amsterdam Baroque plays this movement in 6:43 minutes - the Bach Collegium Japan takes a much slower tempo: 8:33 minutes. 

This chromagram is made using the Dynamic Time-Warping technique to align both versions: a great help if you want to compare various performances. You can see by the longer y-axis that the Bach Collegium Japan plays much slower, resulting in a rectangular chromagram.

From the bottom left corner to the upper right corner you see a line, showing the warping path. A straight line shows that both movements are well aligned. When you look close, you see a small discontinuation of the line towards the end of the movement. This is caused by a cadenza: the flute plays a short cadenza of 16 seconds and the cello plays a more extensive cadenza of 32 seconds - there is no alignment possible here. 

### **Conclusions - TO DO: adjust title depending on other visualisations**.

>

#### **Conclusions etc.** ####

Enter bodytext
